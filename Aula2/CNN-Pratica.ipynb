{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from numpy import vstack\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical \n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import Sigmoid\n",
    "from torch.nn import Module\n",
    "from torch.optim import SGD\n",
    "from torch.nn import BCELoss\n",
    "from torch.nn import MSELoss\n",
    "from torch.nn.init import kaiming_uniform_\n",
    "from torch.nn.init import xavier_uniform_\n",
    "import torch\n",
    "\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import models\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n",
    "from tensorflow.keras.utils import to_categorical \n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desenvolvendo modelo CNN\n",
    "* Modelo pytorch somente tensores\n",
    "* Modelo pytorch com dataloader e dataset\n",
    "* Conversão NHWC para NCHW\n",
    "* Pytorch dataloader, dataset e transformer para data augmentation\n",
    "* Usando modelo pronto para predição\n",
    "* Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset abelhas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ls /home/silvio/git/datasets/bees/wingsEval\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "\n",
    "# Percorre os arquivos no diretório de imagens para treinamento\n",
    "def create_test_data(path):\n",
    "    for p in os.listdir(path):\n",
    "\n",
    "        # categoria da imagem é definida pelo nome do arquivo\n",
    "        category = p.split(\" \")[0]\n",
    "        \n",
    "        # Abre a imagem usando opencv em escala de cinza\n",
    "        #img_array = cv2.imread(os.path.join(path,p),cv2.IMREAD_GRAYSCALE)\n",
    "        img_array = cv2.imread(os.path.join(path,p),cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Redimensionamento para 80 x 80 pixels\n",
    "        new_img_array = cv2.resize(img_array, dsize=(320, 320))\n",
    "        \n",
    "        X.append(new_img_array)\n",
    "        y.append(category)\n",
    "        \n",
    "path=\"/home/silvio/git/datasets/bees/wingsEval\"\n",
    "\n",
    "create_test_data(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# criando tensores de entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "aux=le.fit_transform(y)\n",
    "\n",
    "tX = Tensor(X)\n",
    "ty = Tensor(aux)\n",
    "#X=np.array(X)\n",
    "#y=np.array(y)\n",
    "\n",
    "print(tX.shape)\n",
    "print(ty.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# alterando de NHWC para NCHW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX = tX.permute(0, 3, 1, 2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clase convnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.drop_out = nn.Dropout()\n",
    "        #self.fc1 = nn.Linear(7 * 7 * 64, 1000)\n",
    "        #self.fc2 = nn.Linear(1000, 26)\n",
    "        #100x409600\n",
    "        self.fc1 = nn.Linear(409600, 100)\n",
    "        self.fc2 = nn.Linear(100, 26)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.drop_out(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rotina de treino (Usando tensores)\n",
    "\n",
    "```\n",
    "batche_size=100\n",
    "n_batches=18\n",
    "max_epochs=2\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    for i in range(n_batches):\n",
    "        local_X, local_y = tX[i*batche_size:(i+1)*batche_size,], ty[i*batche_size:(i+1)*batche_size,]\n",
    "        #print(epoch,i,local_X.shape)\n",
    "        #print(epoch,i,local_y.shape)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batche_size=100\n",
    "n_batches=18\n",
    "max_epochs=20\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = ConvNet()\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_list = []\n",
    "acc_list = []\n",
    "\n",
    "# Train model\n",
    "for epoch in range(max_epochs):\n",
    "    for i in range(n_batches):\n",
    "        local_X, local_y = tX[i*batche_size:(i+1)*batche_size,], ty[i*batche_size:(i+1)*batche_size,]\n",
    "\n",
    "        outputs = model(local_X)\n",
    "        \n",
    "        local_y = local_y.to(torch.long)\n",
    "        \n",
    "        loss = criterion(outputs, local_y)\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        # Backprop and perform Adam optimisation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track the accuracy\n",
    "        total = local_y.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct = (predicted == local_y).sum().item()\n",
    "        acc_list.append(correct / total)\n",
    "\n",
    "        print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n",
    "            .format(epoch + 1, max_epochs, i + 1, n_batches, loss.item(),\n",
    "                (correct / total) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rotina de treino com dataset e dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 6\n",
    "num_classes = 30\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* declarando dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class wingsDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, path):\n",
    "\n",
    "        Xl = []\n",
    "        yl = []\n",
    "        \n",
    "        for p in os.listdir(path):\n",
    "            # categoria da imagem é definida pelo nome do arquivo\n",
    "            category = p.split(\" \")[0]\n",
    "        \n",
    "            # Abre a imagem usando opencv em escala de cinza\n",
    "            #img_array = cv2.imread(os.path.join(path,p),cv2.IMREAD_GRAYSCALE)\n",
    "            # Redimensionamento para 28 x 28 pixels\n",
    "            #new_img_array = cv2.resize(img_array, dsize=(28, 28))\n",
    "\n",
    "            img_array = cv2.imread(os.path.join(path,p),cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Redimensionamento para 80 x 80 pixels\n",
    "            new_img_array = cv2.resize(img_array, dsize=(320, 320))            \n",
    "            \n",
    "            Xl.append(new_img_array)\n",
    "            yl.append(category)\n",
    "            #print(p)\n",
    "        le = LabelEncoder()\n",
    "        yll= le.fit_transform(yl)\n",
    "        \n",
    "        #yll=yll.astype(double)\n",
    "        #print(yll)\n",
    "        Xl=np.array(Xl)\n",
    "        \n",
    "        print(Xl.shape)\n",
    "        Xl = np.array(Xl).reshape(-1, 320, 320,3)\n",
    "        print(Xl.shape)\n",
    "        \n",
    "        self.X = Tensor(Xl)\n",
    "        self.y = Tensor(np.array(yll))\n",
    "        print(self.X.shape)\n",
    "        print(torch.unique(self.y))\n",
    "        self.X = self.X.permute(0, 3, 1, 2) # from NHWC to NCHW\n",
    "         \n",
    "    # quantas linhas tem no dataset?\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    " \n",
    "    # obtem uma linha do dataset\n",
    "    def __getitem__(self, idx):\n",
    "        print(idx, self.X.shape)\n",
    "        return [self.X[idx], self.y[idx]]\n",
    " \n",
    "    # retorna base para treino e teste\n",
    "    def get_splits(self, n_test=0.33):\n",
    "        test_size = round(n_test * len(self.X))\n",
    "        train_size = len(self.X) - test_size\n",
    "        return random_split(self, [train_size, test_size])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* instanciando dataset e dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/silvio/dataset/bees/wingsEval/'\n",
    "#path = '/home/silvio/dataset/bees/wingsEvalT/'\n",
    "# Carrega Dataset   \n",
    "dataset = wingsDataset(path)\n",
    "# realiza split\n",
    "train, test = dataset.get_splits()\n",
    "   \n",
    "# monta data loaders\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /home/silvio/dataset/bees/wingsEvalT/ | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* treino do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet()\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "#print(total_step)\n",
    "loss_list = []\n",
    "acc_list = []\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Run the forward pass\n",
    "        #print(images.shape, labels.shape)\n",
    "        labels = labels.to(torch.long)\n",
    "\n",
    "        outputs = model(images)\n",
    "        #print(outputs)\n",
    "        #print(labels)\n",
    "        #print(torch.unique(labels))\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        # Backprop and perform Adam optimisation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track the accuracy\n",
    "        total = labels.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        acc_list.append(correct / total)\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n",
    "                  .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(),\n",
    "                          (correct / total) * 100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pyplot.plot(loss_list, label='train')\n",
    "#pyplot.plot(acc_list, label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.plot(acc_list, label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataloader exemplo gato - cachorro - data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usando modelo para predição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.applications.vgg16 import decode_predictions\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "\n",
    "\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.applications import Xception # TensorFlow ONLY\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.applications import imagenet_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16()\n",
    "#model = VGG19()\n",
    "#model = InceptionV3()\n",
    "#model = Xception()\n",
    "#model = ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = load_img('/home/silvio/mug.jpg', target_size=(224, 224))\n",
    "image = load_img('/home/silvio/bee.jpg', target_size=(224, 224))\n",
    "#image = load_img('/home/silvio/mug.jpg', target_size=(299, 299))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /home/silvio/mug.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converter pixels em um array\n",
    "image = img_to_array(image)\n",
    "\n",
    "# acertar formato do array para o modelo\n",
    "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "\n",
    "# preprocessamento da imagem\n",
    "image = preprocess_input(image)\n",
    "\n",
    "# calcular probabilidade considerando todos os labels\n",
    "yhat = model.predict(image)\n",
    "\n",
    "# Codificar as probabilidades retornadas nos \"labels\"\n",
    "label = decode_predictions(yhat)\n",
    "\n",
    "for (i, (imagenetID, label, prob)) in enumerate(label[0]):\n",
    "    print(\"{}. {}: {:.2f}%\".format(i + 1, label, prob * 100))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# obtém identificação da imagem (Label)\n",
    "#label = label[0][0]\n",
    "\n",
    "# Classificação\n",
    "#print('%s (%.2f%%)' % (label[1], label[2]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reuso de topologia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = ResNet50(weights='imagenet')\n",
    "\n",
    "img_path = '/home/silvio/mug.jpg'\n",
    "img_path = '/home/silvio/bee.jpg'\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "preds = model.predict(x)\n",
    "\n",
    "#print('Predict:', decode_predictions(preds, top=3)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "import numpy as np\n",
    "\n",
    "model = VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "#img_path = '/home/silvio/mug.jpg'\n",
    "img_path = '/home/silvio/bee.jpg'\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "#features = model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "base_model = VGG19(weights='imagenet')\n",
    "model = Model(inputs=base_model.input, outputs=base_model.get_layer('block4_pool').output)\n",
    "\n",
    "#img_path = '/home/silvio/mug.jpg'\n",
    "img_path = '/home/silvio/bee.jpg'\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "#block4_pool_features = model.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning a partir da VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparando dados para o transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrX=np.array(X)\n",
    "arrX.shape\n",
    "arrX=np.array(X)\n",
    "print(arrX.shape)\n",
    "\n",
    "print(aux.shape)\n",
    "\n",
    "yc = to_categorical(aux) \n",
    "x_train, x_test, y_train, y_test = train_test_split(arrX, yc, test_size=0.2, random_state = 42)\n",
    "print(yc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# acoplando nova camada densa ao final do modelo VGG16\n",
    "* GlobalAveragePooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelo pre treinado\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "\n",
    "# forma de desenho de rede neural baseada em grafo (keras)\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "\n",
    "predictions = Dense(25, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "\n",
    "history=model.fit(  x_train,   y_train,   epochs=5,   validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# heatmap da ativação por camadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import applications\n",
    "\n",
    "vgg_model = applications.VGG16(weights='imagenet', include_top=True)\n",
    "vgg_model = applications.VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "from tensorflow.keras.layers import Input\n",
    "input_tensor = Input(shape=(160, 160, 3))\n",
    "vgg_model = applications.VGG16(weights='imagenet',\n",
    "                               include_top=False,\n",
    "                               input_tensor=input_tensor)\n",
    "\n",
    "vgg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_model = applications.VGG16(weights='imagenet',\n",
    "                               include_top=False,\n",
    "                               input_shape=(160, 160, 3))\n",
    "\n",
    "layer_dict = dict([(layer.name, layer) for layer in vgg_model.layers])\n",
    "\n",
    "x = layer_dict['block2_pool'].output\n",
    "\n",
    "x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(10, activation='softmax')(x)\n",
    "\n",
    "custom_model = Model(input=vgg_model.input, output=x)\n",
    "\n",
    "for layer in custom_model.layers[:7]:\n",
    "    layer.trainable = False\n",
    "\n",
    "\n",
    "custom_model.compile(loss='categorical_crossentropy',\n",
    "                     optimizer='rmsprop',\n",
    "                     metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 224\n",
    "\n",
    "\n",
    "model = tf.keras.applications.vgg16.VGG16()\n",
    "img_path = '/home/silvio/bee.jpg'\n",
    "im = image.load_img(img_path, target_size=(224, 224))\n",
    "\n",
    "\n",
    "img_tensor = preprocessing.image.img_to_array(im)\n",
    "img_tensor = np.expand_dims(img_tensor, axis=0)\n",
    "img_tensor = preprocess_input(img_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(im);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layer = model.get_layer(\"block5_conv3\")\n",
    "heatmap_model = models.Model([model.inputs], [conv_layer.output, model.output])\n",
    "\n",
    "\n",
    "with tf.GradientTape() as gtape:\n",
    "    conv_output, predictions = heatmap_model(img_tensor)\n",
    "    loss = predictions[:, np.argmax(predictions[0])]\n",
    "    grads = gtape.gradient(loss, conv_output)\n",
    "    pooled_grads = K.mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "heatmap = tf.reduce_mean(tf.multiply(pooled_grads, conv_output), axis=-1)\n",
    "heatmap = np.maximum(heatmap, 0)\n",
    "max_heat = np.max(heatmap)\n",
    "if max_heat == 0:\n",
    "    max_heat = 1e-10\n",
    "heatmap /= max_heat\n",
    "\n",
    "print(heatmap.shape)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "hm=np.squeeze(heatmap)\n",
    "print(hm.shape)\n",
    "\n",
    "plt.imshow(hm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /home/silvio/bee.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imagem original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('/home/silvio/bee.jpg')\n",
    "\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# heatmap da ativação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm2=hm\n",
    "\n",
    "hm2 = cv2.merge([hm2,hm2,hm2])\n",
    "hm2 = cv2.resize(hm2, (img.shape[1], img.shape[0]))\n",
    "hm2 = np.uint8(255 * hm2)\n",
    "hm2 = cv2.applyColorMap(hm2, cv2.COLORMAP_JET)\n",
    "superimposed_img = hm2 * 0.4 + img \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(hm2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# heatmap + original sobreposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(superimposed_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
